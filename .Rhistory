View(anfragen)
setwd("C:/Users/Felix/OneDrive - bwedu/Documents/Studium/AG Breunig/Ben/scraping_felix/R")
austrian <- read.csv("data/indexpagedata", header = TRUE)
austrian <- read.csv("data/indexpagedata.csv", header = TRUE)
View(austrian)
load("C:/Users/Felix/OneDrive - bwedu/Documents/Studium/AG Breunig/Ben/scraping_felix/R/data/proposalpage.RData")
View(indexpagedata)
View(data1)
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, furrr, rvest, ggthemes, lubridate, rio, haven)
test <- read_html("https://www.parlament.gv.at/PAKT/VHG/XXIV/A/A_02177/index.shtml")
test
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]") %>% .[2] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]") %>% .[2] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test %>% html_nodes("div.c_2 p") %>%
html_text()
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test <- read_html("https://www.parlament.gv.at/PAKT/VHG/XXIV/A/A_02363/index.shtml")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test <- read_html("https://www.parlament.gv.at/PAKT/VHG/XXIV/I/I_02548/index.shtml")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.]")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]") %>% .[-1] %>%
str_extract(":.+")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+")
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim()
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T) %>%
str_subset("\\s{3,}", negate = T)
test %>% html_nodes("div.c_2 p") %>%
html_text() %>%
str_remove_all("[^A-Za-z0-9 ÄÖÜäöü:.-]") %>% .[-1] %>%
str_extract(":.+") %>%
str_extract("[A-Z].+") %>%
str_trim() %>%
str_subset("[0-9]{3}", negate = T)
knitr::opts_chunk$set(echo = TRUE)
all_indexpagedata <- dir("data", pattern = "_X", full.names = T) %>%
map_dfr(readr::read_csv) %>%
select(-X1)
readr::write_csv(all_indexpagedata, "data/indexpagedata.csv")
all_indexpagedata <- dir("data", pattern = "_X", full.names = T) %>%
map_dfr(readr::read_csv) %>%
select(-X1)
readr::write_csv(all_indexpagedata, "data/indexpagedata.csv")
indexpagedata <- readr::read_csv(file = "data/indexpagedata.csv") %>%
mutate_all(as.character)
proposalpagedata <- indexpagedata %>%
mutate(.id = 1:n()) %>%
# sample_n(10) %>%
split(1:nrow(.)) %>%
# I put the scraping code in a function, which is sourced from the script utils.R
# Map is more efficient than for loop
# It take the  providedfunction and apply it to each element of the input
map_dfr(get_meta_info)
proposalpagedata
proposalpagedata <- indexpagedata %>%
mutate(.id = 1:n()) %>%
# sample_n(10) %>%
split(1:nrow(.)) %>%
# I put the scraping code in a function, which is sourced from the script utils.R
# Map is more efficient than for loop
# It take the  providedfunction and apply it to each element of the input
map_dfr(get_meta_info)
get_meta_info
devtools::install_github("saschagobel/legislatoR")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("saschagobel/legislatoR")
devtools::install_github("saschagobel/legislatoR")
get_de
get_de()
library(legislatoR)
get_de()
get_political()
get_political_de
get_political(de)
get_political("de")
get_political("deu")
legis_de <- as_tibble(get_political("deu"))
legis_de
get_core(deu)
get_core("deu")
add_column(legis_de, get_core("deu"))
legis_de <- as_tibble(get_core("deu"))
legis_de
anfragen
anfragen$Anfragesteller
legis_de
legis_de <- left_join(x = legis_de,
y = get_political(legislature = "deu"),
by = "pageid")
legis_de
getOption("repos")
install.packages("caTools")
install.packages("installr")
installr::updateR()
install.packages("caTools")
